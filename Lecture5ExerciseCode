# %%
# Data wrangling
import pandas as pd
import numpy as np

# Data visualisation
import seaborn as sns
import matplotlib.pyplot as plt

# Machine learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

url = 'https://raw.githubusercontent.com/Kbertolu/Lecture4Exercise/refs/heads/main/HW1_Consolidated_Data'
# Read File:    
dataset = pd.read_csv(url, sep = '\t')
# print(dataset)
# data=pd.DataFrame(dataset.data,columns = dataset.featurenames)

# %%
#Data Info:
dataset.info()

#Summary Statistics:
print(dataset.describe().transpose())

# %%
#Applciation:
# Filter for material A and set A
filtered_data = dataset[(dataset['mat'] == 'A') & (dataset['set'] == 'A')]

# Create separate dataframes for each test
data_100N = filtered_data[filtered_data['test'] == '100N'][['result', 'volfrac']]
data_500N = filtered_data[filtered_data['test'] == '500N'][['result', 'volfrac']]
data_1mm = filtered_data[filtered_data['test'] == '1mm'][['result', 'volfrac']]
data_5mm = filtered_data[filtered_data['test'] == '5mm'][['result', 'volfrac']]

# Create the new dataframe
data = pd.DataFrame({
    '100N': data_100N['result'].values,
    '500N': data_500N['result'].values,
    '1mm': data_1mm['result'].values,
    '5mm': data_5mm['result'].values,
    'volfrac': data_100N['volfrac'].values  # We can use any of the volfrac columns as they're the same
})
# Display the result
print("\nNew Table:")
print("-" * 50)
print(data)

# %%
# Application: choose predictor (independent) and target (response) variables
X = data.drop('volfrac', axis = 1) #independent/predictor
Y = data['volfrac'] # repsonse/target

# X, Y shape
print("X shape: ", X.shape)
print("Y shape: ", Y.shape)

# %%
# Instantiate MinMaxScaler, StandardScaler and RobustScaler

norm = MinMaxScaler()
standard = StandardScaler()
robust = RobustScaler()

# MinMaxScaler
normalised_features = norm.fit_transform(X)
normalised_df = pd.DataFrame(normalised_features, index = X.index, columns = X.columns)

# StandardScaler
standardised_features = standard.fit_transform(X)
standardised_df = pd.DataFrame(standardised_features, index = X.index, columns = X.columns)

# RobustScaler
robust_features = robust.fit_transform(X)
robust_df = pd.DataFrame(robust_features, index = X.index, columns = X.columns)

# %%
# Create subplots
fig, ax = plt.subplots(2, 2, figsize = (12, 9))

# Original
sns.boxplot(x = 'variable', y = 'value', data = pd.melt(data[['100N', '500N', '1mm', '5mm']]), ax = ax[0, 0])
ax[0, 0].set_title('Original')
ax[0, 0].set_xlabel('')
ax[0, 0].set_ylabel('')

# MinMaxScaler
sns.boxplot(x = 'variable', y = 'value', data = pd.melt(normalised_df[['100N', '500N', '1mm', '5mm']]), ax = ax[0, 1])
ax[0, 1].set_title('MinMaxScaler')
ax[0, 1].set_xlabel('')
ax[0, 1].set_ylabel('')

# StandardScaler
sns.boxplot(x = 'variable', y = 'value', data = pd.melt(standardised_df[['100N', '500N', '1mm', '5mm']]), ax = ax[1, 0])
ax[1, 0].set_title('StandardScaler')
ax[1, 0].set_xlabel('')
ax[1, 0].set_ylabel('')

# RobustScaler
sns.boxplot(x = 'variable', y = 'value', data = pd.melt(robust_df[['100N', '500N', '1mm', '5mm']]), ax = ax[1, 1])
ax[1, 1].set_title('RobustScaler')
ax[1, 1].set_xlabel('')
ax[1, 1].set_ylabel('')

# %% [markdown]
# 5.2 proof of concept: We should expect to see an improved model performance with feature scaling under KNN and SVR and a constant model performance under decision trees with and without feature scaling.

# %%
# Instantiate models 
knn = KNeighborsRegressor()
svr = SVR()
tree = DecisionTreeRegressor(max_depth = 10, random_state = 42)

# Create a list which contains different scalers 
scalers = [norm, standard, robust]

# %%
# Train test split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)

print("X_train shape: ", X_train.shape)
print("Y_train shape: ", Y_train.shape)
print("X_test shape: ", X_test.shape)
print("Y_test shape: ", Y_test.shape)

# %% [markdown]
# KNN:

# %%
knn_rmse = []

# Without feature scaling
knn.fit(X_train, Y_train)
pred = knn.predict(X_test)
knn_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))

# Apply different scaling techniques and make predictions using KNN 
for scaler in scalers:
    pipe = make_pipeline(scaler, knn)
    pipe.fit(X_train, Y_train)
    pred = pipe.predict(X_test)
    knn_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))

# Show results     
knn_df = pd.DataFrame({'RMSE': knn_rmse}, index = ['Original', 'MinMaxScaler', 'StandardScaler', 'RobustScaler'])
knn_df

# %% [markdown]
# SVR:

# %%
svr_rmse = []

# Without feature scaling
svr.fit(X_train, Y_train)
pred = svr.predict(X_test)
svr_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))

# Apply different scaling techniques and make predictions using SVR
for scaler in scalers:
    pipe = make_pipeline(scaler, svr)
    pipe.fit(X_train, Y_train)
    pred = pipe.predict(X_test)
    svr_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))

# Show results
svr_df = pd.DataFrame({'RMSE': svr_rmse}, index = ['Original', 'MinMaxScaler', 'StandardScaler', 'RobustScaler'])
svr_df

# %% [markdown]
# Decision Tree: 

# %%
tree_rmse = []

# Without feature scaling
tree.fit(X_train, Y_train)
pred = tree.predict(X_test)
tree_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))

# Apply different scaling techniques and make predictions using decision tree
for scaler in scalers:
    pipe = make_pipeline(scaler, tree)
    pipe.fit(X_train, Y_train)
    pred = pipe.predict(X_test)
    tree_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))

# Show results 
tree_df = pd.DataFrame({'RMSE': tree_rmse}, index = ['Original', 'MinMaxScaler', 'StandardScaler', 'RobustScaler'])
tree_df


